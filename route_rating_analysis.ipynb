{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b61fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "import requests\n",
    "import time\n",
    "import geopandas\n",
    "from gpx_converter import Converter\n",
    "from shapely.geometry import LineString\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists as file_exists\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ed92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn import metrics, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2e2db6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-10fc2c05421a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'll'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m've'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pron'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS = STOP_WORDS.union({'ll', 've', 'pron'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2df503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the sinuosity of each route from its gpx file of lat/lon coordinates\n",
    "def calcluate_sinuosity(gpx_file_num):\n",
    "    gpx_file = f'gpx_files/{str(gpx_file_num)}.gpx'\n",
    "    if file_exists(gpx_file):\n",
    "        try:\n",
    "            gpx_array = Converter(input_file=gpx_file).gpx_to_numpy_array()\n",
    "        except Exception:\n",
    "            return -1\n",
    "        \n",
    "        splits = 4\n",
    "        subsets = np.array_split(gpx_array, splits)\n",
    "        subset_sinuosities = []\n",
    "        \n",
    "        for subset in subsets:\n",
    "            start_pt = subset[0]\n",
    "            end_pt = subset[-1]\n",
    "            route = LineString(subset)\n",
    "            route_SL = LineString((start_pt, end_pt))\n",
    "            route_sinuosity = route.length / route_SL.length\n",
    "            subset_sinuosities.append(route_sinuosity)\n",
    "        return sum(subset_sinuosities)/splits\n",
    "    else:\n",
    "        return -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565dfb7",
   "metadata": {},
   "source": [
    "# Data input & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_data_RAW = pd.read_csv('route_data_RAW.csv')\n",
    "s = [calcluate_sinuosity(x) for x in route_data_RAW['gpx_file_num']]\n",
    "route_data_RAW['sinuosity'] = s\n",
    "#route_data_RAW.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc1433",
   "metadata": {},
   "source": [
    "We only want routes that have ratings in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f65ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = (\n",
    "    pd.read_csv('comments.csv')\n",
    "    .drop('files', axis=1)\n",
    "    .dropna()\n",
    "    .groupby('route_name', as_index=False)\n",
    "    .agg(lambda x: ' '.join(x))\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "rated_roads = (\n",
    "    route_data_RAW.query('num_user_reviews > 0 and sinuosity >= 0')\n",
    "    #.merge(comments, how='left', left_on='name',right_on='route_name')\n",
    "    .fillna(' ')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e886f4d",
   "metadata": {},
   "source": [
    "And we will limit scope to routes in the US, including routes that cross state lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255003cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_states = ['Alabama', 'California', 'Georgia', 'Missouri', 'Illinois', 'Ohio',\n",
    "       'Kentucky', 'Colorado', 'United States', 'Indiana', 'New York',\n",
    "       'Vermont', 'Texas', 'Florida', 'Minnesota', 'Virginia',\n",
    "       'Oklahoma', 'Arkansas', 'Maryland', 'West Virginia',\n",
    "       'Michigan', 'North Carolina', 'Oregon', 'Pennsylvania',\n",
    "       'Washington', 'New Jersey', 'Alaska',\n",
    "       'South Carolina', 'Utah', 'New Hampshire', 'Iowa', 'Louisiana',\n",
    "       'Mississippi', 'Wisconsin',\n",
    "       'South Dakota', 'Wyoming', 'Massachusetts', 'New Mexico',\n",
    "       'Montana', 'Idaho', 'Nevada', 'Arizona',\n",
    "       'Kansas', 'Northeast', 'Southwest', 'Golf Coast', 'Southeast',\n",
    "       'Tennessee', 'Nebraska', 'Delaware', 'Pacific Coast',\n",
    "       'Appalachian Mountains', 'Maine', 'Rhode Island', 'Connecticut',\n",
    "       'North Dakota', 'Hawaii']\n",
    "us_route_data = rated_roads[rated_roads.state.isin(valid_states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57262d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#us_route_data['weighted_rating'] = us_route_data['user_rating'] * us_route_data['num_user_reviews']\n",
    "us_route_data['description'] = us_route_data.agg(lambda x: f\"{x['scenery_description']}, {x['drive_enjoyment_description']}, {x['tourism_description']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_route_data['description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a geoDataFrame with route coords as the geometry\n",
    "def get_route_coords(gpx_file_num):\n",
    "    gpx_file = f'gpx_files/{str(gpx_file_num)}.gpx'\n",
    "    if file_exists(gpx_file):\n",
    "        try:\n",
    "            gpx_df = Converter(input_file=gpx_file).gpx_to_dataframe()\n",
    "            route_line = LineString(list(zip(gpx_df.longitude, gpx_df.latitude)))\n",
    "            return route_line\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "route_coords = {\n",
    "    'gpx_file_num': [x for x in us_route_data['gpx_file_num']],\n",
    "    'geometry': [get_route_coords(x) for x in us_route_data['gpx_file_num']]\n",
    "}\n",
    "\n",
    "#also used EPSG: 4326\n",
    "route_coords_gdf = geopandas.GeoDataFrame(route_coords, crs=\"EPSG:4269\").merge(us_route_data, on='gpx_file_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7230b",
   "metadata": {},
   "source": [
    "Dataset of NPS Park boundaries: https://irma.nps.gov/DataStore/Reference/Profile/2225713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83135dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # calculate a route's shortest distance to a NPS site \n",
    "# park_data = geopandas.read_file(\"nps_boundary/nps_boundary.shp\")\n",
    "\n",
    "# route_coords_gdf['centroid'] = route_coords_gdf['geometry'].to_crs(epsg=4269).centroid\n",
    "# park_data['centroid'] = park_data['geometry'].to_crs(epsg=4269).centroid\n",
    "\n",
    "# us_route_data['distance2nps'] = route_coords_gdf.apply(lambda x: park_data['centroid'].distance(x['centroid']).min(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final dataset for analyses\n",
    "route_coords_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042206b",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed140d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "route_coords_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385322a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rode = alt.Chart(route_coords_gdf).mark_point().encode(\n",
    "    x='user_rating',\n",
    "    y='num_users_rode',\n",
    "    tooltip=['name','state','user_rating']\n",
    ").interactive()\n",
    "\n",
    "want2ride = alt.Chart(route_coords_gdf).mark_point().encode(\n",
    "    x='user_rating',\n",
    "    y='num_users_want2ride',\n",
    "    tooltip=['name','state','user_rating']\n",
    ").interactive()\n",
    "\n",
    "rode | want2ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6515d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenery_chart = alt.Chart(rated_roads).mark_circle().encode(\n",
    "    alt.X('scenery_rating', bin=True),\n",
    "    alt.Y('user_rating', bin=True),\n",
    "    size='count()'\n",
    ")\n",
    "\n",
    "drive_enjoyment_chart = alt.Chart(rated_roads).mark_circle().encode(\n",
    "    alt.X('drive_enjoyment_rating', bin=True),\n",
    "    alt.Y('user_rating', bin=True),\n",
    "    size='count()'\n",
    ")\n",
    "\n",
    "tourism_chart = alt.Chart(rated_roads).mark_circle().encode(\n",
    "    alt.X('tourism_rating', bin=True),\n",
    "    alt.Y('user_rating', bin=True),\n",
    "    size='count()'\n",
    ")\n",
    "\n",
    "scenery_chart | drive_enjoyment_chart | tourism_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7818fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(us_route_data).mark_point().encode(\n",
    "    x='distance2nps',\n",
    "    y='user_rating',\n",
    "    tooltip=['name','state','user_rating']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d923a2d",
   "metadata": {},
   "source": [
    "## Geospatial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# US states background\n",
    "states = alt.topo_feature(data.us_10m.url, feature='states')\n",
    "background = alt.Chart(states).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").properties(\n",
    "    width=1000,\n",
    "    height=600\n",
    ").project('albersUsa')\n",
    "\n",
    "# routes\n",
    "lines = alt.Chart(route_coords_gdf).mark_geoshape(\n",
    "    filled=False,\n",
    "    strokeWidth=1\n",
    ").encode(color='user_rating')\n",
    "\n",
    "background + lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173acc32",
   "metadata": {},
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4400c80",
   "metadata": {},
   "source": [
    "## Numerical features & Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_coords_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load/split data\n",
    "X = route_coords_gdf\n",
    "y = route_coords_gdf['weighted_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "# select features\n",
    "numeric_features = ['sinuosity']#,'distance2nps']\n",
    "text_features = ['scenery_description','drive_enjoyment_description','tourism_description']\n",
    "\n",
    "# preprocessing\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", RobustScaler())]\n",
    ")\n",
    "text_transformer = TfidfVectorizer(stop_words=STOP_WORDS.union({'10'}), \n",
    "                                   ngram_range=(1,2),\n",
    "                                   min_df=.05\n",
    "                                  )\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    #('comments',text_transformer, 'comments'),\n",
    "#     ('scenery', text_transformer, 'scenery_description'),\n",
    "#     ('drive_enjoyment', text_transformer, 'drive_enjoyment_description'),\n",
    "#     ('tourism', text_transformer, 'tourism_description')\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline\n",
    "est = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RidgeCV())\n",
    "])\n",
    "\n",
    "est.fit(X_train,y_train)\n",
    "y_pred = est.predict(X_train)\n",
    "print(\"Mean absolute error:\", metrics.mean_absolute_error(y_train, y_pred))\n",
    "print(\"Mean squared error:\", metrics.mean_squared_error(y_train, y_pred))\n",
    "print(\"R^2:\", metrics.r2_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1699cbb",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7ad17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split routes into hi and low ratings\n",
    "sorted_ratings = us_route_data.sort_values(by=['user_rating','num_user_reviews'],ascending=False).reset_index()\n",
    "hi,mid,low = np.split(sorted_ratings,[int(.3*len(sorted_ratings)), int(.6*len(sorted_ratings))])\n",
    "\n",
    "# add rating labels\n",
    "hi['rating_label'] = 'hi'\n",
    "low['rating_label'] = 'low'\n",
    "\n",
    "# merge labeled data\n",
    "polar_data = pd.concat([hi,low],ignore_index=True)\n",
    "polar_X = polar_data['comments']\n",
    "polar_y = polar_data['rating_label']\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(polar_X, polar_y, test_size=0.2, random_state=17)\n",
    "\n",
    "\n",
    "polar_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=STOP_WORDS.union({'10'}), \n",
    "                                   ngram_range=(1,2),\n",
    "                                   min_df=.1, \n",
    "                                   max_features = 500)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "polar_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91453efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_pipe[0].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = polar_pipe.get_params()['vectorizer'].vocabulary_ \n",
    "                                                  \n",
    "coeff_pos = polar_pipe.get_params()['classifier'].feature_log_prob_[0] \n",
    "coeff_neg = polar_pipe.get_params()['classifier'].feature_log_prob_[1]\n",
    "\n",
    "\n",
    "from numpy import argsort\n",
    "\n",
    "polarity = coeff_pos - coeff_neg\n",
    "indices = argsort(polarity) # indices of the polarity list, sorted from least to greatest\n",
    "\n",
    "\n",
    "print(\"Top Words \\n-----\")\n",
    "for word in vocab:\n",
    "    if vocab[word] in indices[-25:]:\n",
    "        print(word)\n",
    "        \n",
    "# print(\"\\nNegative Words \\n-----\")\n",
    "# for word in vocab:\n",
    "#     if vocab[word] in indices[:25]:\n",
    "#         print(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps routes w/ parks\n",
    "\n",
    "# US states background\n",
    "states = alt.topo_feature(data.us_10m.url, feature='states')\n",
    "background = alt.Chart(states).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").properties(\n",
    "    width=1000,\n",
    "    height=600\n",
    ").project('albersUsa')\n",
    "\n",
    "# MR routes\n",
    "lines = alt.Chart(route_coords_gdf).mark_geoshape(\n",
    "    filled=False,\n",
    "    strokeWidth=1\n",
    ").encode(color='user_rating')\n",
    "\n",
    "\n",
    "# NPS sites\n",
    "parks = alt.Chart(park_data).mark_geoshape(\n",
    "        color='brown',\n",
    "        filled=False,\n",
    "        strokeWidth=1)\n",
    "\n",
    "\n",
    "background + lines + parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling on comments to generate 'category' features for each route\n",
    "# recommendation engine for routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f541530",
   "metadata": {},
   "source": [
    "# Topic-modeling route comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1409f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "comments = pd.read_csv('comments.csv')[['comments']].dropna().drop_duplicates()\n",
    "comments['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b69f89",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f103f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner','tagger', 'textcat', 'lemmatizer'])\n",
    "\n",
    "def my_lemmatizer(doc):\n",
    "    doc_cleaned = ' '.join(re.findall(r'\\b\\w[\\w\\']+\\b', doc))\n",
    "    return [ w.lemma_.lower() for w in nlp(doc_cleaned) \n",
    "                      if w.lemma_ not in ['_', '.', '-PRON-'] ]\n",
    "\n",
    "# stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# stopwords = set(my_lemmatizer(' '.join(list(stopwords))))\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.union({'ll', 've', 'pron',\n",
    "                               'good','great','nice',\n",
    "                               'ride', 'route','road'\n",
    "                              })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6888a68",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d09107",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "\n",
    "cv = CountVectorizer(#tokenizer=my_lemmatizer, \n",
    "                     stop_words=STOP_WORDS, \n",
    "                     ngram_range=(1, 3), \n",
    "                     min_df=0.1, \n",
    "                     max_df=0.95, \n",
    "                     #max_features=max_features\n",
    "                    )\n",
    "counts = cv.fit_transform(comments['comments'])\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "matrix = tf.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_of_clusters = range(2,11)\n",
    "\n",
    "for n in range_of_clusters:\n",
    "    kmc = KMeans(n_clusters=n, n_init=3, random_state=17)  # random_state for consistency\n",
    "    kmc.fit(matrix)\n",
    "\n",
    "    number_of_top_words = 10\n",
    "\n",
    "    cluster_words = np.argsort(kmc.cluster_centers_, axis=1)\n",
    "    terms = cv.get_feature_names_out()\n",
    "\n",
    "    top_words = set()\n",
    "\n",
    "    for i in range(n):\n",
    "    #     print('Cluster {}: '.format(i))\n",
    "    #     print(', '.join([terms[k] for k in cluster_words[i][-number_of_top_words:]]),'\\n')\n",
    "        top_words = top_words.union([terms[k] for k in cluster_words[i][-number_of_top_words:]])\n",
    "\n",
    "    top_words = sorted(list(top_words))\n",
    "\n",
    "    #combine word counts with cluster labels\n",
    "    terms = cv.get_feature_names_out()\n",
    "    word_df = pd.DataFrame(counts.toarray(), columns=terms)[top_words]\n",
    "    word_df['Cluster'] = kmc.labels_.tolist()\n",
    "\n",
    "    print(f'Clusters: {n}  Silhouette score: {silhouette_score(matrix, word_df.Cluster)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit \n",
    "number_of_clusters = 5\n",
    "\n",
    "kmc = KMeans(n_clusters=number_of_clusters, n_init=3, random_state=17)  # random_state for consistency\n",
    "kmc.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11740ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_top_words = 7\n",
    "\n",
    "cluster_words = np.argsort(kmc.cluster_centers_, axis=1)\n",
    "terms = cv.get_feature_names_out()\n",
    "\n",
    "top_words = set()\n",
    "\n",
    "for i in range(number_of_clusters):\n",
    "    print('Cluster {}: '.format(i))\n",
    "    print(', '.join([terms[k] for k in cluster_words[i][-number_of_top_words:]]),'\\n')\n",
    "    top_words = top_words.union([terms[k] for k in cluster_words[i][-number_of_top_words:]])\n",
    "    \n",
    "top_words = sorted(list(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06aae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=117)\n",
    "matrix_pca = pca.fit_transform(matrix.toarray())\n",
    "\n",
    "matrix_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.scatter(matrix_pca[:,0], matrix_pca[:,1], c=word_df['Cluster'], \n",
    "            cmap='viridis', alpha=0.15)\n",
    "plt.gca().set(title='Plot for 2-Dimensional PCA Projection', \n",
    "              xlabel='PCA component 1', ylabel='PCA component 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.groupby('Cluster').count()[top_words[0]].\\\n",
    "    plot.bar(rot=0).\\\n",
    "    set(ylabel='Document count',\n",
    "    title='Number of Documents per Cluster');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa721b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.groupby('Cluster').sum().transpose().\\\n",
    "    plot.bar(figsize=(13,5), width=0.7).\\\n",
    "    set(ylabel='Word frequency', \n",
    "    title='Word Frequencies by Topic, Combining the Top {} Words in Each Topic'.format(number_of_top_words));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac642f",
   "metadata": {},
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to get gpx files from MotoRoads site\n",
    "\n",
    "# for gpx in gpxs:\n",
    "#     moto = requests.get('https://www.motorcycleroads.com/downloadgpx/' + str(gpx))\n",
    "#     out = moto.text\n",
    "#     name = str(gpx) + '.gpx'\n",
    "#     with open(name, 'w') as f:\n",
    "#         f.write(out)\n",
    "#     time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
